
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Tips for Deep Learning Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../pytorch/pytorch.html" />
    
    
    <link rel="prev" href="deep_learning.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../markdown_math_syntax.html">
            
                <a href="../markdown_math_syntax.html">
            
                    
                    Markdown Math Syntax
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../gradient_descent/gradient_descent.html">
            
                <a href="../gradient_descent/gradient_descent.html">
            
                    
                    Gradient Descent
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../gradient_descent/more_about_gradient_descent.html">
            
                <a href="../gradient_descent/more_about_gradient_descent.html">
            
                    
                    More about Gradient Descent
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../gradient_descent/new_optimization.html">
            
                <a href="../gradient_descent/new_optimization.html">
            
                    
                    New Optimazation
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../classification/classification.html">
            
                <a href="../classification/classification.html">
            
                    
                    Classification
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../logistic_regression/logistic_regression.html">
            
                <a href="../logistic_regression/logistic_regression.html">
            
                    
                    Logistic Regression
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../logistic_regression/lrcode.html">
            
                <a href="../logistic_regression/lrcode.html">
            
                    
                    LRCode
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="deep_learning.html">
            
                <a href="deep_learning.html">
            
                    
                    Deep Learing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.6.1" data-path="tips_for_deep_learning.html">
            
                <a href="tips_for_deep_learning.html">
            
                    
                    Tips for Deep Learning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../pytorch/pytorch.html">
            
                <a href="../pytorch/pytorch.html">
            
                    
                    Pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../pytorch/main_mdel_introduction.html">
            
                <a href="../pytorch/main_mdel_introduction.html">
            
                    
                    Main Model Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../pytorch/three_kinds_of_data.html">
            
                <a href="../pytorch/three_kinds_of_data.html">
            
                    
                    three kinds of data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../pytorch/real_data_representaion.html">
            
                <a href="../pytorch/real_data_representaion.html">
            
                    
                    real world data representation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../pytorch/walking_through_a_learning_algorithm_from_scratch.html">
            
                <a href="../pytorch/walking_through_a_learning_algorithm_from_scratch.html">
            
                    
                    The mechanics of learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../pytorch/autograd.html">
            
                <a href="../pytorch/autograd.html">
            
                    
                    Backpropagate all things
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.6" data-path="../pytorch/training_validation_overfitting.html">
            
                <a href="../pytorch/training_validation_overfitting.html">
            
                    
                    Training, validation, and overfitting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.7" data-path="../pytorch/neural_network.html">
            
                <a href="../pytorch/neural_network.html">
            
                    
                    Neural Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.8" data-path="../pytorch/nn_module.html">
            
                <a href="../pytorch/nn_module.html">
            
                    
                    NN Module
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.9" data-path="../pytorch/subclassing_nn_module.html">
            
                <a href="../pytorch/subclassing_nn_module.html">
            
                    
                    Subclassing NN Module
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.10" data-path="../pytorch/pre-trained_network.html">
            
                <a href="../pytorch/pre-trained_network.html">
            
                    
                    Pre-trained Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.11" data-path="../pytorch/image_classification.html">
            
                <a href="../pytorch/image_classification.html">
            
                    
                    Image Classification
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.12" data-path="../pytorch/problem_set_of_pytorch.html">
            
                <a href="../pytorch/problem_set_of_pytorch.html">
            
                    
                    Problem Set of PyTorch
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../convolutional_neural_network/cnn.html">
            
                <a href="../convolutional_neural_network/cnn.html">
            
                    
                    Convolutional Neural network
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../convolutional_neural_network/cnn_network_structure.html">
            
                <a href="../convolutional_neural_network/cnn_network_structure.html">
            
                    
                    CNN Network Structure
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="../convolutional_neural_network/common_cnn.html">
            
                <a href="../convolutional_neural_network/common_cnn.html">
            
                    
                    Common CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="../convolutional_neural_network/cnn_aplication.html">
            
                <a href="../convolutional_neural_network/cnn_aplication.html">
            
                    
                    CNN Aplication
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../graph_neural_network/graph_neural_network.html">
            
                <a href="../graph_neural_network/graph_neural_network.html">
            
                    
                    Graph Neural Network
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../graph_neural_network/introduce_to_graph_neural_network.html">
            
                <a href="../graph_neural_network/introduce_to_graph_neural_network.html">
            
                    
                    Introduce to Graph Neural Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../graph_neural_network/Spatial_based_gnn.html">
            
                <a href="../graph_neural_network/Spatial_based_gnn.html">
            
                    
                    Spatial-based GNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../graph_neural_network/Spectral_based_gnn.html">
            
                <a href="../graph_neural_network/Spectral_based_gnn.html">
            
                    
                    Spectral_based_gnn
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../recurrent_neural_network/recurrent_neural_network.html">
            
                <a href="../recurrent_neural_network/recurrent_neural_network.html">
            
                    
                    Recurrent Neural Network
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="../recurrent_neural_network/recursive_structure.html">
            
                <a href="../recurrent_neural_network/recursive_structure.html">
            
                    
                    Recursive Structure
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../semi_supervised/semi_supervised.html">
            
                <a href="../semi_supervised/semi_supervised.html">
            
                    
                    Semi-supervised
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="../word_embedding/word_embedding.html">
            
                <a href="../word_embedding/word_embedding.html">
            
                    
                    Word Embedding
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13" data-path="../explainable_machine_learning/explainable_machine_learning.html">
            
                <a href="../explainable_machine_learning/explainable_machine_learning.html">
            
                    
                    Explainable Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="../attack_ml_models/attack_ml_models.html">
            
                <a href="../attack_ml_models/attack_ml_models.html">
            
                    
                    Attack ML Models
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.14.1" data-path="../attack_ml_models/attack_image_and_audio.html">
            
                <a href="../attack_ml_models/attack_image_and_audio.html">
            
                    
                    Attack image and audio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="../network_compression/network_compression.html">
            
                <a href="../network_compression/network_compression.html">
            
                    
                    Network Compression
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="../network_compression/knowledge_distillation.html">
            
                <a href="../network_compression/knowledge_distillation.html">
            
                    
                    Knowledge Distillation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="../network_compression/network_pruning.html">
            
                <a href="../network_compression/network_pruning.html">
            
                    
                    Network Pruning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="../transformer/transformer.html">
            
                <a href="../transformer/transformer.html">
            
                    
                    Transformer
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.16.1" data-path="../transformer/variant_of_transformers.html">
            
                <a href="../transformer/variant_of_transformers.html">
            
                    
                    Variant of Transformers
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.17" data-path="../conditional_generation_by_RNN_&_Attention/conditional_generation_by_RNN_&_Attention.html">
            
                <a href="../conditional_generation_by_RNN_&_Attention/conditional_generation_by_RNN_&_Attention.html">
            
                    
                    Conditional Generation by RNN & Attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18" data-path="../unsupervised_learning/unsupervised_learning.html">
            
                <a href="../unsupervised_learning/unsupervised_learning.html">
            
                    
                    Unsupervised Learning
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.18.1" data-path="../unsupervised_learning/dimension_reduction.html">
            
                <a href="../unsupervised_learning/dimension_reduction.html">
            
                    
                    Dimension Reduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18.2" data-path="../unsupervised_learning/neighbor_embedding.html">
            
                <a href="../unsupervised_learning/neighbor_embedding.html">
            
                    
                    Neighbor Embedding
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18.3" data-path="../unsupervised_learning/deep_auto_encoder.html">
            
                <a href="../unsupervised_learning/deep_auto_encoder.html">
            
                    
                    Deep Auto-encoder
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.19" data-path="../bert_and_its_family/bert_and_its_family.html">
            
                <a href="../bert_and_its_family/bert_and_its_family.html">
            
                    
                    Bert and its Family
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.19.1" data-path="../bert_and_its_family/bert.html">
            
                <a href="../bert_and_its_family/bert.html">
            
                    
                    Bert
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.20" data-path="../nlp/natural_language_processing.html">
            
                <a href="../nlp/natural_language_processing.html">
            
                    
                    Natual Language Processing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.20.1" data-path="../nlp/introduction.html">
            
                <a href="../nlp/introduction.html">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.21" data-path="../anomaly_detection/anomaly_detection.html">
            
                <a href="../anomaly_detection/anomaly_detection.html">
            
                    
                    Anomaly Detection
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Tips for Deep Learning</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="tips-for-deep-learning">Tips for Deep Learning</h1>
<h2 id="recipe-of-deep-learning">Recipe of Deep Learning</h2>
<p><em>overfitting</em> is based on testing set, sometimes you have to check the training set. </p>
<table>
<thead>
<tr>
<th>Training Data</th>
<th>Testing Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>New Activation</td>
<td>Early Stopping</td>
</tr>
<tr>
<td>Adaptive Learning Rate</td>
<td>Regularization</td>
</tr>
<tr>
<td></td>
<td>Dropout</td>
</tr>
</tbody>
</table>
<h2 id="keep-in-mind">Keep in mind</h2>
<p>When you see a method in the literature of deep learning, always think about what kind of problem this method is to solve, because in deep learning, there are two problems.</p>
<ul>
<li>The performance on the training set is not good enough</li>
<li>The performance on the test set is not good enough  </li>
</ul>
<p>When there is a method proposed, it often tends to handle only one of the two problems. So, you have to figure out what the problem is, and then find a targeted method based on this problem.</p>
<h2 id="vanishing-gradient-problem">Vanishing Gradient Problem</h2>
<p>When the inputs of the sigmoid function becomes larger or smaller, the derivative becomes close to zero. When more layers are used, it can cause the gradient to be too small for training to work effectively.<br>A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network. </p>
<p><strong>Solution</strong></p>
<ul>
<li>ReLU (Rectified Linear Unit)<ul>
<li>Fast to compute</li>
<li>Infinite sigmoid with different biases</li>
<li>handle vanishing gradient problem, do not have smaller gradient</li>
</ul>
</li>
</ul>
<p><script type="math/tex; ">\sigma(z) =a = \begin{cases}
    z,\qquad  z>0 \\\\
    0,\qquad z\leq0
\end{cases}</script></p>
<ul>
<li>Residual networks<ul>
<li>provide residual connections straight to earlier layers</li>
<li>doesn&#x2019;t go through activation functions that &#x201C;squashes&#x201D; the derivatives, resulting in a higher overall derivative of the block</li>
</ul>
</li>
<li>batch normalization layers</li>
</ul>
<h3 id="maxout">Maxout</h3>
<ul>
<li>Given a training data x, we know which z would be the max<ul>
<li>Arbitrarily group $z_1^1,z_2^1,\cdots$, such as ${z_1^1,z_2^1}$</li>
<li>$\max {z_1^1,z_2^1} \rightarrow a_1^1$</li>
</ul>
</li>
<li>Learnable Activation Function  <ul>
<li>Activation function in maxout network can be any piecewise linear convex funtion.</li>
<li>How many pieces depending on how many elements in a group.</li>
</ul>
</li>
<li>Different thin and linear network for different training examples.</li>
<li>ReLU is a special case of Maxout.</li>
<li>Output <strong>a</strong> is produced by max operation rather than activation function.</li>
</ul>
<p>In specific practice, we can first convert the max function to a specific function based on data, and then differentiate the converted thinner linear network.</p>
<h3 id="adagrad">Adagrad</h3>
<p>If gradient is small, set a larger learning rate; If gradient is large, set a smaller learning rate.<br><script type="math/tex; ">w^{t+1} \leftarrow w^t - \dfrac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}}g^{t}</script></p>
<p>Adagrad considers the average gradient information of the entire process.</p>
<h3 id="rmsprop">RMSProp</h3>
<p>Adjust the learning rate rapidly according to the gradient at any time (advanced Adagrad)
<script type="math/tex; ">w^{t+1} = w^t - \dfrac{\eta}{\sigma^t}g^t</script>
<script type="math/tex; ">\sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1 - \alpha)(g^t)^2}</script>
<script type="math/tex; ">\sigma^0 = g^0</script></p>
<p>RMSProp dynamicly tunes incidence of gradient by $\alpha$ at different times.<br>(If you set $\alpha$ smaller, RMSProp is tent to believe present gradient whether is slippery or Steep in order to set larger or smaller gradient rather than last gradient)</p>
<p><strong>Gradient Update</strong>  </p>
<p>$w^1 \leftarrow w^0 - \dfrac{\eta}{\sigma^0} \qquad \sigma^0 = g^0$</p>
<p>$w^2 \leftarrow w^1 - \dfrac{\eta}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\alpha(\sigma^0)^2 + (1 - \alpha)(g^1)^2}$</p>
<p>$w^3 \leftarrow w^2 - \dfrac{\eta}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\alpha(\sigma^1)^2 + (1 - \alpha)(g^2)^2}$</p>
<p>$\vdots$</p>
<p>$w^{t+1} \leftarrow w^t - \dfrac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1 - \alpha)(g^t)^2}$</p>
<p>Root Mean Square of the gradients with previous gradients being decayed  </p>
<h2 id="momentum">Momentum</h2>
<p>Movement of last step minus gradient at present</p>
<ul>
<li>Start at point $\theta^0$</li>
<li>Initial movement $v^0 = 0$</li>
<li>Compute gradient at $\theta^0$</li>
<li>Movement $v^1 = \lambda v^0 - \eta\nabla L(\theta^0)$</li>
<li>Move to $\theta^1 = \theta^0 + v^1$</li>
<li>Compute gradient at $\theta^1$</li>
<li>Movement $v^2 = \lambda v^1 - \eta\nabla L(\theta^1)$</li>
<li>Move to $\theta^2 = \theta^1 + v^2$</li>
</ul>
<p>Movement not just based on gradient,but previous movement.<br>$v^i$ is actually the weighted sum of all the previous gradient:</p>
<ul>
<li>$v^0 = 0$</li>
<li>$v^1 = - \eta\nabla L(\theta^0)$</li>
<li>$v^2 = -\lambda \eta \nabla L(\theta^0) - \eta \nabla L(\theta^1)$</li>
</ul>
<h2 id="momentum-vs-adagrad">Momentum V.s. AdaGrad</h2>
<p>Momentum adds updates to the slope of our error function and speeds up SGD in turn. AdaGrad adapts updates to each individual parameter to perform larger or smaller updates depending on their importance</p>
<h2 id="adam">Adam</h2>
<ul>
<li>$m_0 = 0$, it is previous movement in Momentum</li>
<li>$v_0 = 0$, it is $\sigma$ in the root mean square of the gradient calculated in RMSProp</li>
<li>$t = 0$, it means moment.</li>
<li>calculate gradient $g_t$
<script type="math/tex; ">g_t = \nabla_\theta f_t(\theta_{t-1})</script></li>
<li>calculate $m<em>t$ by $m</em>{t-1}$ and $g_t$ ------ Momentum
<script type="math/tex; ">m_t = \beta_1m_{t-1} + (1 - \beta_1)g_t</script></li>
<li>calculate $v<em>t$ by $v</em>{t-1}$ and $g_t$ ------ RMSProp
<script type="math/tex; ">v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2</script></li>
<li>bias correction 
<script type="math/tex; ">\hat{m}_t = \dfrac{m_t}{1-\beta_1^t}</script>
<script type="math/tex; ">\hat{v}_t = \dfrac{v_t}{1-\beta_2^t}</script></li>
<li>Update
<script type="math/tex; ">\theta_t = \theta_{t-1} - \dfrac{\alpha\cdot\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}</script></li>
</ul>
<p><strong>Adam = RMSProp + Momentum</strong></p>
<h2 id="regularization">Regularization</h2>
<p>New loss function to be minimized</p>
<ul>
<li>Find a set of weight not only minimizing original cost but also close to zero</li>
</ul>
<blockquote>
<p>L-P Norm
<script type="math/tex; ">L_p = \sqrt[^p\!]{\sum_{i=1}^nx_i^p} \quad,x = (x_1,x_2,\cdots,x_n) </script></p>
<p>L0 Norm : P = 0<br>mainly used to measure the number of non-zero elements in the vector </p>
<p>L1 Norm : P =1<br>the solution optimized for L1 is a sparse solution, so the L1 norm is also called the sparse rule operator. L1 can achieve sparse features, remove some features without information.
<script type="math/tex; ">\left\|\theta\right\|_1 = \sum_i|x_i|</script></p>
<p>L2 Norm : P = 2<br>The L2 norm is usually used as the regularization term of the optimization goal, to prevent the model from being too complicated in order to cater to the training set and cause overfitting, thereby improving the generalization ability of the model
<script type="math/tex; ">\left\|\theta\right\|_2 = \sqrt{\sum_ix_i^2}</script></p>
</blockquote>
<p>$\theta = {w_1,w_2,\cdots}$   </p>
<h3 id="l2-regularization-l2-norm">L2 regularization (L2 Norm)</h3>
<p><script type="math/tex; ">L^\prime(\theta) = L(\theta) + \lambda\frac{1}{2}\left\|\theta\right\|_2</script></p>
<p><script type="math/tex; ">\left\|\theta\right\|_2 = (w_1)^2 + (w_2)^2 + \cdots</script></p>
<p><strong>Gradient update</strong>
<script type="math/tex; ">\dfrac{\partial L^\prime(\theta)}{\partial w} = \dfrac{\partial L(\theta)}{\partial w} + \lambda w</script>
<script type="math/tex; ">w^{t+1} \leftarrow w^t - \eta \dfrac{\partial L^\prime(\theta)}{\partial w} = w^t - \eta \left(\dfrac{\partial L(\theta)}{\partial w} + \lambda w^t\right)</script>
<script type="math/tex; ">\qquad = (1 - \eta\lambda)w^t - \eta\dfrac{\partial L(\theta)}{\partial w}</script>
It is obviously that exits weight decay.</p>
<h3 id="l1-regularizationl1-norm">L1 regularization(L1 Norm)</h3>
<p><script type="math/tex; ">L^\prime(\theta) = L(\theta) + \lambda\frac{1}{2}\left\|\theta\right\|_1</script>
<script type="math/tex; ">\left\|\theta\right\|_1 = |w_1| + |w_2| + \cdots</script>  </p>
<p><strong>Gradient Update</strong>
<script type="math/tex; ">\dfrac{\partial L^\prime(\theta)}{\partial w} = \dfrac{\partial L(\theta)}{\partial w} + \lambda\, sgn(w)</script>
<script type="math/tex; ">sgn(w) = \begin{cases}
    1,\qquad\quad w>0 \\
    -1,\qquad w<0 \\
\end{cases}</script>
<script type="math/tex; ">w^{t+1} \leftarrow w^t - \eta \dfrac{\partial L^\prime(\theta)}{\partial w} = w^t - \eta \left( \dfrac{\partial L(\theta)}{\partial w} + \lambda\, sgn(w^t)\right)</script></p>
<p><script type="math/tex; ">\qquad = w^t - \eta\dfrac{\partial L(\theta)}{\partial w} - \eta \lambda sgn(w^t)w</script></p>
<h3 id="l1-vs-l2">L1 V.s. L2</h3>
<p>Although they also make the absolute value of the parameter smaller, they actually do slightly different things:</p>
<ul>
<li>L1 makes the absolute value of the parameter smaller by subtracting a fixed value for every update</li>
<li>L2 makes the absolute value of the parameter smaller by multiplying by a fixed value less than 1 for every update.</li>
</ul>
<h2 id="dropout">Dropout</h2>
<ul>
<li>During training, each time before updating the parameters, we do sampling for each neuron (also including the input layer). Each neuron has a p% chance that it will be discarded (dropout). If it is lost, the weight connected to it will also be lost.</li>
<li>The parameters between different networks are shared.</li>
<li>Assuming that during training, the dropout rate is p%, all weights learned from the training data must be multiplied by (1-p%) to be used as the weight of the testing set.</li>
</ul>
<p>What Dropout really wants to do is to make your results on the training set worse, but the results on the testing set are better.Dropout is a optimization method for testing set but is used for training .</p>
<p>If the type of network is very close to linear, the performance of dropout will be better, and the network of ReLU and Maxout is relatively close to linear, so we usually use the network with ReLU or Maxout in Dropout.</p>
<h3 id="maxout-vs-dropout">Maxout V.s. Dropout</h3>
<p>Maxout is that the network structure corresponding to each data is different, and Dropout is that the network structure is different for each update (each minibatch corresponds to an update, and a minibatch contains many data)</p>
<h2 id="parameter-initialization">Parameter Initialization</h2>
<p>What should be the scale of this initialization? If we choose large values for the weights, this can lead to exploding gradients. On the other hand, small values for weights can lead to vanishing gradients. There is some sweet spot that provides the optimum tradeoff between these two, but it cannot be known a priori and must be inferred through trial and error.</p>
<p>In general, it is a good idea to avoid presupposing any form of a neural structure by randomizing weights according to a normal distribution.</p>
<h3 id="xavier-initialization">Xavier Initialization</h3>
<p>Xavier initialization is a simple heuristic for assigning network weights. With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to high values or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for both the input and the output.</p>
<p>The weights are drawn from a distribution with zero mean and a specific variance. For a fully-connected layer with m inputs:
<script type="math/tex; ">W_{ij} \sim N\left(0,\frac{1}{m}\right)</script>
The value m is sometimes called the fan-in: the number of incoming neurons (input units in the weight tensor).</p>
<h3 id="he-normal-initialization">He Normal Initialization</h3>
<p>He normal initialization is essentially the same as Xavier initialization, except that the variance is multiplied by a factor of two.The weights are still random but differ in range depending on the size of the previous layer of neurons. This provides a controlled initialization hence the faster and more efficient gradient descent.</p>
<p>For ReLU units, it is recommended:
<script type="math/tex; ">W_{ij} \sim N\left(0,\frac{2}{m}\right)</script></p>
<h2 id="bias-initialization">Bias Initialization</h2>
<p>The simplest and a common way of initializing biases is to set them to zero.</p>
<p>One main concern with bias initialization is to avoid saturation at initialization within hidden units &#x2014; this can be done, for example in ReLU, by initializing biases to 0.1 instead of zero.</p>
<h2 id="pre-initialization">Pre-Initialization</h2>
<p> This is common for convolutional networks used for examining images, which is to be used on similar data to that which the network was trained on.</p>
<h2 id="feature-normalization">Feature Normalization</h2>
<p>manipulate the data itself in order to aid our model optimization</p>
<ol>
<li><p>min-max normalization<br>rescaling the range of features to scale the range in [0, 1] or [&#x2212;1, 1].
As this has a tendency to collapse outliers so they have a less profound effect on the distribution.
<script type="math/tex; ">x^\prime = \dfrac{x-\min(x)}{\max(x)-\min(x)}</script></p>
</li>
<li><p>Feature standardization
makes the values of each feature in the data have zero-mean and unit-variance,which ameliorate the distortion (uniforming the elongation of one feature compared to another feature)
<script type="math/tex; ">x^\prime = \dfrac{x - \mu}{\sigma}</script></p>
</li>
</ol>
<h2 id="batch-normalization">Batch Normalization</h2>
<p>Batch normalization is an extension to the idea of feature standardization to other layers of the neural network.</p>
<ul>
<li>To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.<br>$\mu$ is the vector of mean activations across mini-batch, $\sigma$ is the vector of SD of each unit across mini-batch.
<script type="math/tex; ">H^\prime = \dfrac{H - \mu}{\sigma}</script>
<script type="math/tex; ">\mu = \dfrac{1}{m}\sum_iH_i</script>
<script type="math/tex; ">$$
</script>\sigma = \sqrt{\dfrac{1}{m}\sum_i(H - \mu)_i^2+\delta}<script type="math/tex; mode=display">

Batch normalization allows each layer of a network to learn by itself more independently of other layers.

However, after this shift/scale of activation outputs by some randomly initialized parameters, the weights in the next layer are no longer optimal. SGD ( Stochastic gradient descent) undoes this normalization if itâs a way for it to minimize the loss function.

Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a âstandard deviationâ parameter (Î³) and add a âmeanâ parameter (Î²). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights. ($\gamma,\beta$ are learnable)
</script>\gamma H^\prime + \beta$$
This procedure is known as the batch normalization transform.<br>During test time, the mean and standard deviations are replaced with running averages collected during training time,which ensures that the output deterministically depends on the input.<br>Batch normalization reduces overfitting because it has a slight regularization effect.Similar to dropout, it adds some noise to each hidden layer&#x2019;s activations.   </li>
</ul>
<p>There are several advantages to using batch normalization:</p>
<ul>
<li>Reduces internal covariant shift.</li>
<li>Reduces the dependence of gradients on the scale of the parameters or their initial values.</li>
<li>Regularizes the model and reduces the need for dropout, photometric distortions, local response normalization and other regularization techniques.</li>
<li>Allows use of saturating nonlinearities and higher learning rates.</li>
</ul>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="deep_learning.html" class="navigation navigation-prev " aria-label="Previous page: Deep Learing">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../pytorch/pytorch.html" class="navigation navigation-next " aria-label="Next page: Pytorch">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Tips for Deep Learning","level":"1.6.1","depth":2,"next":{"title":"Pytorch","level":"1.7","depth":1,"path":"pytorch/pytorch.md","ref":"pytorch/pytorch.md","articles":[{"title":"Main Model Introduction","level":"1.7.1","depth":2,"path":"pytorch/main_mdel_introduction.md","ref":"pytorch/main_mdel_introduction.md","articles":[]},{"title":"three kinds of data","level":"1.7.2","depth":2,"path":"pytorch/three_kinds_of_data.md","ref":"pytorch/three_kinds_of_data.md","articles":[]},{"title":"real world data representation","level":"1.7.3","depth":2,"path":"pytorch/real_data_representaion.md","ref":"pytorch/real_data_representaion.md","articles":[]},{"title":"The mechanics of learning","level":"1.7.4","depth":2,"path":"pytorch/walking_through_a_learning_algorithm_from_scratch.md","ref":"pytorch/walking_through_a_learning_algorithm_from_scratch.md","articles":[]},{"title":"Backpropagate all things","level":"1.7.5","depth":2,"path":"pytorch/autograd.md","ref":"pytorch/autograd.md","articles":[]},{"title":"Training, validation, and overfitting","level":"1.7.6","depth":2,"path":"pytorch/training_validation_overfitting.md","ref":"pytorch/training_validation_overfitting.md","articles":[]},{"title":"Neural Network","level":"1.7.7","depth":2,"path":"pytorch/neural_network.md","ref":"pytorch/neural_network.md","articles":[]},{"title":"NN Module","level":"1.7.8","depth":2,"path":"pytorch/nn_module.md","ref":"pytorch/nn_module.md","articles":[]},{"title":"Subclassing NN Module","level":"1.7.9","depth":2,"path":"pytorch/subclassing_nn_module.md","ref":"pytorch/subclassing_nn_module.md","articles":[]},{"title":"Pre-trained Network","level":"1.7.10","depth":2,"path":"pytorch/pre-trained_network.md","ref":"pytorch/pre-trained_network.md","articles":[]},{"title":"Image Classification","level":"1.7.11","depth":2,"path":"pytorch/image_classification.md","ref":"pytorch/image_classification.md","articles":[]},{"title":"Problem Set of PyTorch","level":"1.7.12","depth":2,"path":"pytorch/problem_set_of_pytorch.md","ref":"pytorch/problem_set_of_pytorch.md","articles":[]}]},"previous":{"title":"Deep Learing","level":"1.6","depth":1,"path":"deep_learning/deep_learning.md","ref":"deep_learning/deep_learning.md","articles":[{"title":"Tips for Deep Learning","level":"1.6.1","depth":2,"path":"deep_learning/tips_for_deep_learning.md","ref":"deep_learning/tips_for_deep_learning.md","articles":[]}]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["mathjax","katex"],"pluginsConfig":{"mathjax":{"forceSVG":false,"version":"2.6-latest"},"katex":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"deep_learning/tips_for_deep_learning.md","mtime":"2020-07-20T13:54:09.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-07-20T14:05:14.223Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

